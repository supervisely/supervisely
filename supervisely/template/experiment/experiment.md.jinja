# {{ experiment.name }}

*Last trained: {{ experiment.date }}*

[Framework poster: DEIM]

## Quick Actions

    [üöÄ Deploy] [‚ö° Predict] [üì• Download] [üîÑ Finetune]

üî¥üî¥üî¥ –ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–∏ –∫–Ω–æ–ø–∫–∏ –∫–∞–∫ Cards (https://docs.gitbook.com/creating-content/blocks/cards)

### Table of Contents

- [Experiment Overview](#experiment-overview)
- [Performance](#performance)
- [Artifacts](#artifacts)
- [Supervisely Apps](#supervisely-apps)
- [API Integration](#api-integration)
- [Deploy in Docker](#predict-in-docker)
- [Deploy outside of Supervisely](#predict-locally)
- [Using Original Model](#using-original-model)

## Experiment Overview

- [üéì Training Session]({{ links.training_session.url }}){:target="_blank"}
- [üìä Evaluation Report]({{ links.evaluation_report.url }}){:target="_blank"}
- [üìà TensorBoard Logs]({{ links.tensorboard_logs.url }}){:target="_blank"}
- [üìÇ Open in Team Files]({{ links.team_files.url }}){:target="_blank"}

### Training Details:

- **Model**: {{ experiment.model_name }}
- **Framework**: {{ experiment.framework_name }}
- **Task**: {{ experiment.task_name }}
- **Project**: {{ project.name }}
- **Classes**: {{ project.classes_count }} ({{ project.class_names }})
- **Training images**: {{ project.train_size }}
- **Validation images**: {{ project.val_size }}
- **mAP**: 72.1 üî¥üî¥üî¥

{% if artifacts.hyperparameters %}
<details>
<summary>Training hyperparameters</summary>
```yaml
{% for hyperparameter in artifacts.hyperparameters %}
{{ hyperparameter }}
{% endfor %}
```
</details>
{% endif %}

## Prediction Samples
{% if widgets.sample_pred_gallery %}
<div class="prediction-gallery">
{{ widgets.sample_pred_gallery | safe }}
</div>
{% endif %}

## Performance
{% if artifacts.metrics_table %}
### üìà Evaluation Metrics
The **{{ artifacts.best_checkpoint.name }}** checkpoint was evaluated on the validation set of **{{ project.val_size }} images**. 

See the full [üìä Evaluation Report]({{ links.evaluation_report.url }}) for details and visualizations.
{{ artifacts.metrics_table | safe }}
{% endif %}

## Artifacts

[üìÇ Open in Team Files]({{ links.team_files.url }}){:target="_blank"}

- üéØ Best checkpoint: **{{ artifacts.best_checkpoint.name }}** ([download]({{ artifacts.best_checkpoint.url }}){:download="{{ artifacts.best_checkpoint.name }}"})
{% if artifacts.onnx_checkpoint.name %}
- üì¶ ONNX export: **{{ artifacts.onnx_checkpoint.name }}** ([download]({{ artifacts.onnx_checkpoint.url }}){:download="{{ artifacts.onnx_checkpoint.name }}"})
{% endif %}
{% if artifacts.trt_checkpoint.name %}
- üì¶ TensorRT export: **{{ artifacts.trt_checkpoint.name }}** ([download]({{ artifacts.trt_checkpoint.url }}){:download="{{ artifacts.trt_checkpoint.name }}"})
{% endif %}

{% if artifacts.checkpoints_table %}
<details>
<summary>üìã All Checkpoints</summary>
{{ artifacts.checkpoints_table }}
</details>
{% endif %}

## Supervisely Apps

These apps will help you to work with your model in the Supervisely Platform. You can use them to deploy your model, make predictions, and train a model.

- [Serve {{ experiment.framework_name }}](xxx) - deploy your model in the Supervisely Platform.
- [Train {{ experiment.framework_name }}](xxx) - train a model in the Supervisely Platform.
- [Apply NN to Images]({{ env.server_address }}/ecosystem/apps/nn-image-labeling/project-dataset) - connect to your model and make predictions on image project or dataset.
- [Apply NN to Videos]({{ env.server_address }}/ecosystem/apps/apply-nn-to-videos-project) - for predictions on video project or dataset.

## API Integration

You can use **Supervisely API** to deploy your model in the Platform and make predictions in your own code. This is a great way to integrate the model into your application.

### Quickstart with API

This is a quick guide to get you started with the Supervisely API.

1. Install Supervisely:

    ```bash
    pip install supervisely
    ```

2. Authentication. Provide your **API token** and the **Server address** into environment variables. For example, you can pass them in the terminal before running the script:

    ```bash
    export API_TOKEN="your_api_token"
    export SERVER_ADDRESS="https://app.supervisely.com"  # or your own server URL for Enterprise Edition
    ```

    If you need help with authentication, check the [Basics of Authentication](https://developer.supervisely.com/getting-started/basics-of-authentication) tutorial.

3. The following code snippet demonstrates how to deploy a model and make predictions using the Supervisely API.

    ```python
    import supervisely as sly

    # 1. Authenticate with Supervisely API
    api = sly.Api()  # Make sure you've set your credentials in environment variables.

    # 2. Deploy the model
    model = api.nn.deploy(
        model="{{ experiment.artifacts_dir }}/checkpoints/{{ artifacts.best_checkpoint.name }}",
        device="cuda:0",  # or "cpu"
    )
    # 3. Predict
    predictions = model.predict(
        input=["image1.jpg", "image2.jpg"],  # can also be numpy arrays, PIL images, URLs or a directory
    )
    ```

## Deploy via API

You can deploy your model in a few lines of code. The model will be deployed in the Supervisely Platform, and you can use it for predictions.

<div>

<sly-iw-tabs :tabs="[
    { name: 'pytorch', title: 'PyTorch Checkpoint' },
    {% if artifacts.onnx_checkpoint.name %}
    { name: 'onnx',  title: 'ONNX Model'  },
    {% endif %}
    {% if artifacts.trt_checkpoint.name %}
    { name: 'tensorrt',    title: 'TensorRT Model' }
    {% endif %}
    ]" :defaultIndex="0" :command="command" :data="data">

<template #pytorch>

```python
import supervisely as sly

api = sly.Api()

# Deploy PyTorch checkpoint
model = api.nn.deploy(
    model="{{ experiment.artifacts_dir }}/checkpoints/{{ artifacts.best_checkpoint.name }}",
    device="cuda:0",  # or "cpu"
)
```

</template>

{% if artifacts.onnx_checkpoint.name %}

<template #onnx>

```python
import supervisely as sly

api = sly.Api()

# Deploy ONNX checkpoint
model = api.nn.deploy(
    model="{{ experiment.artifacts_dir }}/export/{{ artifacts.onnx_checkpoint.name }}",
    device="cuda:0",  # or "cpu"
)
```
{% endif %}

</template>

{% if artifacts.trt_checkpoint.name %}

<template #tensorrt>

```python
import supervisely as sly

api = sly.Api()

# Deploy TensorRT checkpoint
model = api.nn.deploy(
    model="{{ experiment.artifacts_dir }}/export/{{ artifacts.trt_checkpoint.name }}",
    device="cuda:0",  # or "cpu"
)
```
{% endif %}

</template>

</sly-iw-tabs>

</div>

> For more information, see [Model API](https://docs.supervisely.com/neural-networks/overview-1/model-api) documentation.

### Predict via API

You can use the deployed model to make predictions on images, videos, or directories. First, connect to a deployed model, then you can make predictions.

<div>

<sly-iw-tabs :tabs="[
    { name: 'local', title: 'Local Images' },
    { name: 'image_ids',  title: 'Image IDs'  },
    { name: 'dataset',    title: 'Dataset'    },
    { name: 'project',    title: 'Project'    },
    { name: 'video',      title: 'Video'      }
    ]" :defaultIndex="0" :command="command" :data="data">

<template #local>

```python
# Predict local images
predictions = model.predict(
    input="image.jpg"  # can also be a directory, np.array, PIL.Image, URL or a list of them
)
```

</template>

<template #image_ids>

```python
# Predict images in Supervisely
prediction = model.predict(
    image_ids=[123, 124]  # Image ids in Supervisely
)
```

</template>

<template #dataset>

```python
# Predict dataset
prediction = model.predict(
    dataset_id=12,  # Dataset id in Supervisely
)
```

</template>

<template #project>

```python
# Predict project
prediction = model.predict(
    project_id=21,  # Project id in Supervisely
)
```

</template>

<template #video>

```python
# Predict video
predictions = model.predict(
    video_id=123,  # Video id in Supervisely
)
```

</template>

</sly-iw-tabs>

</div>

> For more information, see [Prediction API](https://docs.supervisely.com/neural-networks/overview-1/prediction-api).

## Tracking Objects in Video

You can track objects in video using `boxmot` library. [BoxMot](https://github.com/mikel-brostrom/boxmot) is a third-party library that implements lightweight neural networks for tracking-by-detection task (when the tracking is performed on the objects predicted by a separate detector). For `boxmot` models you can use even CPU device.

First, install [BoxMot](https://github.com/mikel-brostrom/boxmot):

```bash
pip install boxmot
```

Supervisely SDK has the `track()` method from `supervisely.nn.tracking` which allows you to apply `boxmot` models together with a detector in a single line of code. This method takes two arguments: a `boxmot` tracker, and a `PredictionSession` of a detector. It returns a `sly.VideoAnnotation` with the tracked objects.

```python
import supervisely as sly
from supervisely.nn.tracking import track
import boxmot
from pathlib import Path

# Deploy a detector
detector = api.nn.deploy(
    model="rt-detrv2/RT-DETRv2-M",
    device="cuda:0",  # Use GPU for detection
)

# Load BoxMot tracker
tracker = boxmot.BotSort(
    reid_weights=Path('osnet_x0_25_msmt17.pt'),
    device="cpu",  # Use CPU for tracking
)

# Track objects in a single line
video_ann: sly.VideoAnnotation = track(
    video_id=42,
    detector=detector,
    tracker=tracker,
)
```

> For more information, see the section [Tracking Objects in Video](https://docs.supervisely.com/neural-networks/overview-1/prediction-api#tracking-objects-in-video).

---

## Advanced Usage

## Deploy in Docker

You can deploy the model in a üêã Docker Container with a single `docker run` command. Download a checkpoint, pull the docker image for the corresponding model's framework, and run the `docker run` command with addtional arguments.

1. Download checkpoint from Supervisely - [Open in Team Files]({{ links.checkpoint_dir_url }})

2. Pull the Docker image

    ```bash
    docker pull {{ code.docker.image }}
    ```

3. Run the Docker container

    ```bash
    docker run \
    --runtime=nvidia \
    -v "./{{ artifacts.experiment_dir }}:/model" \
    -p 8000:8000 \
    {{ code.docker.image }} \
    deploy \
    --model "/model/checkpoints/{{ artifacts.best_checkpoint.name }}" \
    --device "cuda:0"
    ```

4. Connect and run the inference:

    ```python
    from supervisely.nn import ModelAPI

    # No need to authenticate for local deployment
    model = ModelAPI(
        url="http://localhost:8000",  # URL of a running model's server in Docker container
    )

    # Predict
    predictions = model.predict(
        input=["image1.jpg", "image2.jpg"],  # can also be numpy arrays, PIL images, URLs or a directory
    )
    ```

Alternatively, you can use `docker run` with the `predict` action to make predictions in a single command. This is a quick way to run inference on your local images, videos, or directories.

```bash
docker run \
  --runtime=nvidia \
  -v "./{{ artifacts.experiment_dir }}:/model" \
  -p 8000:8000 \
  {{ code.docker.image }} \
  predict \
  "./image.jpg" \  # Put your image/video/directory here
  --model "/model/checkpoints/{{ artifacts.best_checkpoint.name }}" \
  --device "cuda:0"
```

> For more information, see [Deploy in Docker Container](https://docs.supervisely.com/neural-networks/overview-1/deploy_and_predict_with_supervisely_sdk#deploy-in-docker-container) documentation.

## Deploy outside of Supervisely

In case you develop an application that is not related to the Supervisely platform, you can use our implementation of the model in your own code. This will allow you to use the model in your own codebase, and you can deploy it in any way you want.

1. Download checkpoint from Supervisely - [Open in Team Files]({{ links.checkpoint_dir_url }})

2. Clone our repository

    ```bash
    git clone {{ code.local_prediction.repo_url }}
    cd {{ code.local_prediction.repo_name }}
    ```

3. Install requirements

    ```bash
    pip install -r dev_requirements.txt
    pip install supervisely
    ```

4. Run the inference code

    ```python
    # Be sure you are in the root of the {{ code.local_prediction.repo_name }} repository
    from {{ code.local_prediction.serving_module }} import {{ code.local_prediction.serving_class }}

    # Load model
    model = {{ code.local_prediction.serving_class }}(
        model="./{{ artifacts.experiment_dir }}/checkpoints/{{ artifacts.best_checkpoint.name }}",  # path to the checkpoint you've downloaded
        device="cuda",  # or "cuda:1", "cpu"
    )

    # Predict
    predictions = model(
        # Input can accpet various formats: image paths, np.arrays, Supervisely IDs and others.
        input=["path/to/image1.jpg", "path/to/image2.jpg"],
        conf=0.5,  # confidence threshold
        # ... additional parameters (see the docs)
    )
    ```

> For more information, see [Prediction API](https://docs.supervisely.com/neural-networks/overview-1/prediction-api) and [Local Deployment](https://docs.supervisely.com/neural-networks/overview-1/local-deployment.md) documentation.

{% if experiment.export %}
#### Deploy ONNX/TensorRT

You can also use exported ONNX and TensorRT models. Specify the `model` parameter as a path to your ONNX or TensorRT model, and provide class names in the additional `classes` parameter.  üî¥üî¥üî¥ classes - Not Implemented yet

```python
# Be sure you are in the root of the {{ code.local_prediction.repo_name }} repository
from {{ code.local_prediction.serving_module }} import {{ code.local_prediction.serving_class }}

classes = üî¥üî¥üî¥

# Deploy ONNX or TensorRT
model = {{ code.local_prediction.serving_class }}(
    model="./{{ artifacts.experiment_dir }}/{{ artifacts.onnx_checkpoint.name or artifacts.tensorrt_checkpoint.name }}",  # path to the ONNX or TensorRT model"
    device="cuda",
    classes=classes,
)

# Predict
predictions = model(
    # Input can accpet various formats: image paths, np.arrays, Supervisely IDs and others.
    input=["path/to/image1.jpg", "path/to/image2.jpg"],
    conf=0.5,  # confidence threshold
    # ... additional parameters (see the docs)
)
```

{% endif %}

## Using Original Model

In this approach you'll completely decouple your model from both the **Supervisely Platform** and **Supervisely SDK**, and you will develop your own code for inference and deployment of that particular model. It's important to understand that for each neural network or a framework, you need to set up an environment and write inference code by yourself, since each model has its own installation instructions and the way of processing inputs and outputs correctly.

We provide a basic instructions and a demo script of how to load {{ experiment.framework_name }} and get predictions using the original code from the authors.

1. Download checkpoint from Supervisely - [Open in Team Files]({{ links.checkpoint_dir_url }})

2. Prepare environment following the instructions of the original repository [{{ code.local_prediction.repo_name }}]({{ code.local_prediction.repo_url }})

3. Use the demo script for inference:

üî¥üî¥üî¥ TODO: we don't have the demo script as a variable

<details>
<summary>Click to expand</summary>

```python
"""Copyright(c) 2023 lyuwenyu. All Rights Reserved.
"""

import torch
import torch.nn as nn 
import torchvision.transforms as T

import numpy as np 
from PIL import Image, ImageDraw

from src.core import YAMLConfig


def draw(images, labels, boxes, scores, thrh = 0.6):
    for i, im in enumerate(images):
        draw = ImageDraw.Draw(im)

        scr = scores[i]
        lab = labels[i][scr > thrh]
        box = boxes[i][scr > thrh]
        scrs = scores[i][scr > thrh]

        for j,b in enumerate(box):
            draw.rectangle(list(b), outline='red',)
            draw.text((b[0], b[1]), text=f"{lab[j].item()} {round(scrs[j].item(),2)}", fill='blue', )

        im.save(f'results_{i}.jpg')


def main(args, ):
    """main
    """
    cfg = YAMLConfig(args.config, resume=args.resume)

    if args.resume:
        checkpoint = torch.load(args.resume, map_location='cpu') 
        if 'ema' in checkpoint:
            state = checkpoint['ema']['module']
        else:
            state = checkpoint['model']
    else:
        raise AttributeError('Only support resume to load model.state_dict by now.')

    # NOTE load train mode state -> convert to deploy mode
    cfg.model.load_state_dict(state)

    class Model(nn.Module):
        def __init__(self, ) -> None:
            super().__init__()
            self.model = cfg.model.deploy()
            self.postprocessor = cfg.postprocessor.deploy()
            
        def forward(self, images, orig_target_sizes):
            outputs = self.model(images)
            outputs = self.postprocessor(outputs, orig_target_sizes)
            return outputs

    model = Model().to(args.device)

    im_pil = Image.open(args.im_file).convert('RGB')
    w, h = im_pil.size
    orig_size = torch.tensor([w, h])[None].to(args.device)

    transforms = T.Compose([
        T.Resize((640, 640)),
        T.ToTensor(),
    ])
    im_data = transforms(im_pil)[None].to(args.device)

    output = model(im_data, orig_size)
    labels, boxes, scores = output

    draw([im_pil], labels, boxes, scores)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config', type=str, )
    parser.add_argument('-r', '--resume', type=str, )
    parser.add_argument('-f', '--im-file', type=str, )
    parser.add_argument('-d', '--device', type=str, default='cpu')
    args = parser.parse_args()
    main(args)
```

</details>

---

[‚ùå Remove permamently button]
