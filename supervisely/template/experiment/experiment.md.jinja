# {{ experiment.name }}

*Last trained: {{ experiment.date }}*

[Framework poster: DEIM]

## Quick Actions

    [🚀 Deploy] [⚡ Predict] [📥 Download] [🔄 Finetune]

### Table of Contents

- [Experiment Overview](#experiment-overview)
- [Performance](#performance)
- [Artifacts](#artifacts)
- [API Integration](#api-integration)
- [Advanced Usage](#advanced-usage)
- [Deploy in Docker](#predict-in-docker)
- [Deploy outside of Supervisely](#predict-locally)
- [Using Original Model](#using-original-model)

## Experiment Overview

- <a href="{{ links.training_session.url }}" target="_blank">🎓 Training Session</a>
- <a href="{{ links.evaluation_report.url }}" target="_blank">📊 Evaluation Report</a>
- <a href="{{ links.tensorboard_logs.url }}" target="_blank">📈 TensorBoard Logs</a>
- <a href="{{ links.team_files.url }}" target="_blank">📂 Open in Team Files</a>

### Training Details:

- **Model**: {{ experiment.model_name }}
- **Framework**: {{ experiment.framework_name }}
- **Task**: {{ experiment.task_name }}
- **Project**: {{ project.name }}
- **Classes**: {{ project.classes_count }} ({{ project.class_names }})
- **Training images**: {{ project.train_size }}
- **Validation images**: {{ project.val_size }}
- **mAP**: 72.1 🔴🔴🔴

{% if artifacts.hyperparameters %}
<details>
<summary>Training hyperparameters</summary>
```yaml
{% for hyperparameter in artifacts.hyperparameters %}
{{ hyperparameter }}
{% endfor %}
```
</details>
{% endif %}

## Prediction Samples
🔴🔴🔴 Implement prediction gallery widget

## Performance
{% if artifacts.metrics_table %}
### 📈 Evaluation Metrics
The **{{ artifacts.best_checkpoint }}** checkpoint was evaluated on the validation set of **{{ project.val_size }} images**.
{{ artifacts.metrics_table | safe }}
{% endif %}

## Artifacts

<a href="{{ links.team_files.url }}" target="_blank">📂 Open in Team Files</a>

- 🎯 Best checkpoint: **{{ artifacts.best_checkpoint }}** ([download](xxx))
{# {% if ... %} #}
- 📦 ONNX export: **{ artifacts.onnx_checkpoint }** ([download](xxx))
- 📦 TensorRT export: **{ artifacts.tensorrt_checkpoint }** ([download](xxx))
{# {% endif %} #}

{% if artifacts.checkpoints_table %}
<details>
<summary>📋 All Checkpoints</summary>
{{ artifacts.checkpoints_table }}
</details>
{% endif %}

---

## API Integration

### Quickstart example

This is a quick example of how to deploy the model and make predictions using the Supervisely API.

```python
import supervisely as sly

# 1. Authenticate with Supervisely API
api = sly.Api()

# 2. Deploy the model
model = api.nn.deploy(
    model="{{ experiment.artifacts_dir }}/checkpoints/{{ artifacts.best_checkpoint }}",
    device="cuda:0",  # or "cpu"
)
# 3. Predict
predictions = model.predict(
    input=["image1.jpg", "image2.jpg"],  # can also be numpy arrays, PIL images, URLs or a directory
)
```

### Deploy

You can deploy your model in a few lines of code. The model will be deployed in the Supervisely Platform, and you can use it for predictions.

#### Deploy best checkpoint 🔴🔴🔴 TABS WIDGET

```python
import supervisely as sly

api = sly.Api()

# Deploy
model = api.nn.deploy(
    model="{{ experiment.artifacts_dir }}/checkpoints/{{ artifacts.best_checkpoint }}",
    device="cuda:0",  # or "cpu"
)
```

#### Deploy ONNX model 🔴🔴🔴 TABS WIDGET

```python
import supervisely as sly

api = sly.Api()

# Deploy ONNX checkpoint
model = api.nn.deploy(
    model="{{ experiment.artifacts_dir }}/{{ artifacts.optimized_checkpoint }}",
    device="cuda:0",  # or "cpu"
)
```

#### Deploy TensorRT model 🔴🔴🔴 TABS WIDGET

```python
import supervisely as sly

api = sly.Api()

# Deploy TensorRT checkpoint
model = api.nn.deploy(
    model="{{ experiment.artifacts_dir }}/{{ artifacts.optimized_checkpoint }}",
    device="cuda:0",  # or "cpu"
)
```

> For more information, see [Model API](https://docs.supervisely.com/neural-networks/overview-1/model-api) documentation.

### Predict

You can use the deployed model to make predictions on images, videos, or directories. First, connect to a deployed model, then you can make predictions.

🔴🔴🔴 TABS WIDGET

#### Local Images

```python
# Predict local images
predictions = model.predict(
    input="image.jpg"  # can also be a directory, np.array, PIL.Image, URL or a list of them
)
```

<details>
<summary>📋 TABS WIDGET</summary>

#### Image ID 🔴🔴🔴 TABS WIDGET

```python
# Predict images in Supervisely
prediction = model.predict(
    image_ids=[123, 124]  # Image ids in Supervisely
)
```

#### Dataset 🔴🔴🔴 TABS WIDGET

```python
# Predict dataset
prediction = model.predict(
    dataset_id=12,  # Dataset id in Supervisely
)
```

#### Project 🔴🔴🔴 TABS WIDGET

```python
# Predict project
prediction = model.predict(
    project_id=21,  # Project id in Supervisely
)
```

#### Video 🔴🔴🔴 TABS WIDGET

```python
# Predict video
predictions = model.predict(
    video_id=123,  # Video id in Supervisely
)
```

</details>

> For more information, see [Prediction API](https://docs.supervisely.com/neural-networks/overview-1/prediction-api).

### Tracking Objects in Video

You can track objects in video using `boxmot` library. [BoxMot](https://github.com/mikel-brostrom/boxmot) is a third-party library that implements lightweight neural networks for tracking-by-detection task (when the tracking is performed on the objects predicted by a separate detector). For `boxmot` models you can use even CPU device.

First, install [BoxMot](https://github.com/mikel-brostrom/boxmot):

```bash
pip install boxmot
```

Supervisely SDK has the `track()` method from `supervisely.nn.tracking` which allows you to apply `boxmot` models together with a detector in a single line of code. This method takes two arguments: a `boxmot` tracker, and a `PredictionSession` of a detector. It returns a `sly.VideoAnnotation` with the tracked objects.

```python
import supervisely as sly
from supervisely.nn.tracking import track
import boxmot
from pathlib import Path

# Deploy a detector
detector = api.nn.deploy(
    model="rt-detrv2/RT-DETRv2-M",
    device="cuda:0",  # Use GPU for detection
)

# Load BoxMot tracker
tracker = boxmot.BotSort(
    reid_weights=Path('osnet_x0_25_msmt17.pt'),
    device="cpu",  # Use CPU for tracking
)

# Track objects in a single line
video_ann: sly.VideoAnnotation = track(
    video_id=42,
    detector=detector,
    tracker=tracker,
)
```

> For more information, see the section [Tracking Objects in Video](https://docs.supervisely.com/neural-networks/overview-1/prediction-api#tracking-objects-in-video).

---

## Advanced Usage

### Deploy in Docker

You can apply this model in a 🐋 Docker Container with a single `docker run` comand. For this, you need to download a checkpoint, pull the docker image for the corresponding model's framework, and run the `docker run` comand with addtional arguments.

1. Download checkpoint from Supervisely - [Open in Team Files]({{ links.checkpoint_dir_url }})

2. Pull the Docker image

```bash
docker pull {{ code.docker.image }}
```

3. Run the Docker container

```bash
docker run \
  --runtime=nvidia \
  -v "./{{ artifacts.experiment_dir }}:/model" \
  -p 8000:8000 \
  {{ code.docker.image }} \
  predict \
  "./image.jpg" \  # Put your image/video/directory here
  --model "/model/checkpoints/{{ artifacts.best_checkpoint }}" \
  --device "cuda:0" \
```

> For more information, see [Deploy in Docker Container](https://docs.supervisely.com/neural-networks/overview-1/deploy_and_predict_with_supervisely_sdk#deploy-in-docker-container) documentation.

### Deploy outside of Supervisely

In the case of local deployment, the model will be deployed outside of Supervisely Platform. This is useful when you're developing applications that are not directly related to the platform, and you can just use the model itself in your code.

Here's how to deploy the model locally:

1. Download checkpoint from Supervisely - [Open in Team Files]({{ links.checkpoint_dir_url }})

2. Clone our repository

```bash
git clone {{ code.local_prediction.repo_url }}
cd {{ code.local_prediction.repo_name }}
```

3. Install requirements

```bash
pip install -r dev_requirements.txt
pip install supervisely
```

4. Run the inference code

```python
# Be sure you are in the root of the {{ code.local_prediction.repo_name }} repository
from {{ code.local_prediction.serving_module }} import {{ code.local_prediction.serving_class }}

# Load model
model = {{ code.local_prediction.serving_class }}(
    model="./{{ artifacts.experiment_dir }}/checkpoints/{{ artifacts.best_checkpoint }}",  # path to the checkpoint you've downloaded
    device="cuda",  # or "cuda:1", "cpu"
)

# Predict
predictions = model(
    # Input can accpet various formats: image paths, np.arrays, Supervisely IDs and others.
    input=["path/to/image1.jpg", "path/to/image2.jpg"],
    conf=0.5,  # confidence threshold
    # ... additional parameters (see the docs)
)
```

> For more information, see [Prediction API](https://docs.supervisely.com/neural-networks/overview-1/prediction-api) and [Local Deployment](https://docs.supervisely.com/neural-networks/overview-1/local-deployment.md) documentation.

{% if experiment.export %}
#### Deploy ONNX/TensorRT

You can also use exported ONNX and TensorRT models. Specify the `model` parameter as a path to your ONNX or TensorRT model, and provide class names in the additional `classes` parameter.  🔴🔴🔴 classes - Not Implemented yet

```python
# Be sure you are in the root of the {{ code.local_prediction.repo_name }} repository
from {{ code.local_prediction.serving_module }} import {{ code.local_prediction.serving_class }}

classes = 🔴🔴🔴

# Deploy ONNX or TensorRT
model = {{ code.local_prediction.serving_class }}(
    model="./{{ artifacts.experiment_dir }}/{{ artifacts.optimized_checkpoint }}",  # path to the ONNX or TensorRT model"
    device="cuda",
    classes=classes,
)

# Predict
predictions = model(
    # Input can accpet various formats: image paths, np.arrays, Supervisely IDs and others.
    input=["path/to/image1.jpg", "path/to/image2.jpg"],
    conf=0.5,  # confidence threshold
    # ... additional parameters (see the docs)
)
```

{% endif %}

### Using Original Model

In this approach you'll completely decouple your model from both the **Supervisely Platform** and **Supervisely SDK**, and you will develop your own code for inference and deployment of that particular model. It's important to understand that for each neural network or a framework, you need to set up an environment and write inference code by yourself, since each model has its own installation instructions and the way of processing inputs and outputs correctly.

We provide a basic instructions and a demo script of how to load {{ experiment.framework_name }} and get predictions using the original code from the authors.

1. Download checkpoint from Supervisely - [Open in Team Files]({{ links.checkpoint_dir_url }})

2. Prepare environment following the instructions of the original repository [{{ code.local_prediction.repo_name }}]({{ code.local_prediction.repo_url }})

3. Use the demo script for inference:

🔴🔴🔴 TODO: we don't have the demo script as a variable

<details>
<summary>Click to expand</summary>

```python
"""Copyright(c) 2023 lyuwenyu. All Rights Reserved.
"""

import torch
import torch.nn as nn 
import torchvision.transforms as T

import numpy as np 
from PIL import Image, ImageDraw

from src.core import YAMLConfig


def draw(images, labels, boxes, scores, thrh = 0.6):
    for i, im in enumerate(images):
        draw = ImageDraw.Draw(im)

        scr = scores[i]
        lab = labels[i][scr > thrh]
        box = boxes[i][scr > thrh]
        scrs = scores[i][scr > thrh]

        for j,b in enumerate(box):
            draw.rectangle(list(b), outline='red',)
            draw.text((b[0], b[1]), text=f"{lab[j].item()} {round(scrs[j].item(),2)}", fill='blue', )

        im.save(f'results_{i}.jpg')


def main(args, ):
    """main
    """
    cfg = YAMLConfig(args.config, resume=args.resume)

    if args.resume:
        checkpoint = torch.load(args.resume, map_location='cpu') 
        if 'ema' in checkpoint:
            state = checkpoint['ema']['module']
        else:
            state = checkpoint['model']
    else:
        raise AttributeError('Only support resume to load model.state_dict by now.')

    # NOTE load train mode state -> convert to deploy mode
    cfg.model.load_state_dict(state)

    class Model(nn.Module):
        def __init__(self, ) -> None:
            super().__init__()
            self.model = cfg.model.deploy()
            self.postprocessor = cfg.postprocessor.deploy()
            
        def forward(self, images, orig_target_sizes):
            outputs = self.model(images)
            outputs = self.postprocessor(outputs, orig_target_sizes)
            return outputs

    model = Model().to(args.device)

    im_pil = Image.open(args.im_file).convert('RGB')
    w, h = im_pil.size
    orig_size = torch.tensor([w, h])[None].to(args.device)

    transforms = T.Compose([
        T.Resize((640, 640)),
        T.ToTensor(),
    ])
    im_data = transforms(im_pil)[None].to(args.device)

    output = model(im_data, orig_size)
    labels, boxes, scores = output

    draw([im_pil], labels, boxes, scores)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config', type=str, )
    parser.add_argument('-r', '--resume', type=str, )
    parser.add_argument('-f', '--im-file', type=str, )
    parser.add_argument('-d', '--device', type=str, default='cpu')
    args = parser.parse_args()
    main(args)
```

</details>

---

[❌ Remove permamently button]
