{% markdown %}

# {{ experiment.name }}
    
## Overview

- **Model**: {{ experiment.model_name }}
- **Task**: {{ experiment.task_name }}
- **Framework**: {{ experiment.framework_name }}
- **Project**: {{ project.name }}
- **Train size**: {{ project.train_size }} {{ project.type }}
- **Validation size**: {{ project.val_size }} {{ project.type }}
- **Classes**: {{ project.classes_count }}
- **Class names**: {{ project.class_names }}
- **Date**: {{ experiment.date }}

## Links

- <a href="{{ links.training_session.url }}" target="_blank">üéìTraining Session</a>
- <a href="{{ links.evaluation_report.url }}" target="_blank">üìäEvaluation Report</a>
- <a href="{{ links.tensorboard_logs.url }}" target="_blank">‚ö°TensorBoard Logs</a>
- <a href="{{ links.team_files.url }}" target="_blank">üíæOpen in Team Files</a>

        
## Buttons

- üöÄ Deploy (PyTorch)
{% if buttons.has_onnx %}
- üöÄ Deploy (ONNX)
{% endif %}
{% if buttons.has_tensorrt %}
- üöÄ Deploy (TensorRT)
{% endif %}
- ‚è©Fine-tune
- üîÑRe-train
- üì¶Download model
- ‚ùåRemove permamently

## Checkpoints

{% if artifacts.checkpoints_table %}
{{ artifacts.checkpoints_table }}
{% endif %}

## Metrics
{% if artifacts.metrics_table %}
{{ artifacts.metrics_table | safe }}
{% endif %}

## Training Hyperparameters
{% if artifacts.hyperparameters %}
<details>
    <summary>Click to expand hyperparameters</summary>
    ```yaml
    {% for hyperparameter in artifacts.hyperparameters %}
    {{ hyperparameter }}
    {% endfor %}
    ```
</details>
{% endif %}

## Model API

Deploy and predict in Supervisely.

```python
import supervisely as sly

api = sly.Api()

# Deploy
model = api.nn.deploy_custom_model(checkpoint_id={checkpoint_path}) # file id

# Predict
prediction = model.predict(images="image.png") # image | path | url
```

See more in [Deploy and Predict with Supervisely
SDK](https://docs.supervisely.com/neural-networks/overview-1/deploy_and_predict_with_supervisely_sdk)
documentation.

## Docker


Predict using Docker container.

1. Download checkpoint from Supervisely - [open checkpoint directory]({{ links.checkpoint_dir_url }})
2. Pull the Docker image

    ```bash
    docker pull {{ code.docker.image }}
    ```

3. Run the Docker container

    ```bash
    docker run \
        --runtime=nvidia \
        -v "./{experiment_dir}:/model" \
        -p 8000:8000 \
        {{ code.docker.image }} \
        predict \
        "./image.jpg" \
        --model "/model" \
        --device "cuda:0
    ```

See more in [Deploy in Docker Container](https://docs.supervisely.com/neural-networks/overview-1/deploy_and_predict_with_supervisely_sdk#deploy-in-docker-container) documentation.

## Predict Locally

Predict using local inference.

1. Download checkpoint from Supervisely - [open checkpoint directory]({{ links.checkpoint_dir_url }})
2. Clone repository

    ```bash
    git clone {{ code.local_prediction.repo_url }}
    cd {{ code.local_prediction.repo_name }}
    ```

3. Install requirements

    ```bash
    pip install -r dev_requirements.txt
    pip install supervisely
    ```

4. Run inference

    ```python
    # Be sure you are in the root of the {{ code.local_prediction.repo_name }} repository
    from {{ code.local_prediction.serving_module }} import {{ code.local_prediction.serving_class }}

    # Load model
    model = {{ code.local_prediction.serving_class }}(
        checkpoint="./{{ artifacts.experiment_dir }}/checkpoints/{{ artifacts.best_checkpoint }}", # path to the
        checkpoint
        device="cuda",
    )

    # Predict
    prediction = model(
        "image.png", # local paths, directory, local project, np.array, PIL.Image, URL
        params={"confidence_threshold": 0.5}
    )
    ```

See more in [Predict Locally](https://docs.supervisely.com/neural-networks/overview-1/deploy_and_predict_with_supervisely_sdk#predict-locally) documentation.


{% if artifacts.optimized_checkpoint %}
## Using optimized models
    ```python
    # Be sure you are in the root of the {{ code.local_prediction.repo_name }} repository
    from {{ code.local_prediction.serving_module }} import {{ code.local_prediction.serving_class }}

    model = {{ code.local_prediction.serving_class }}(
        model_dir="./{{ artifacts.experiment_dir }}",
        checkpoint="{{ artifacts.optimized_checkpoint }}",
        device="cuda",
    )
    ```
{% endif %}

{% endmarkdown %}
